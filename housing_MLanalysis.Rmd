---
title: "R Notebook"
output: html_notebook
---

```{r, echo=F}
library(ggplot2)
library(tidyverse)
library(corrplot)
library(e1071)
library(VIM)
library(caret)
library(glmnet)  #LASSO
library(randomForest)  #RF
library(e1071)  # SVR
library(neuralnet) #NN 
library(adabag)  # AdaBoost

setwd("~/NYCDSA/ML_projects/AmesHousing")
```

FUNCTIONS
```{r}
# All variables imputed as "None" were specified in the dataset documentation. 
# "SBrkr" was imputed in Electrical because it is overwhelmingly the most common electrical type.
# MasVnrTypr was imputed as "None" because that was the most common masonry type.

cat_imputation <- function(df){
  
  df.char <- sapply(df, as.character)
  
  df.char <- as.data.frame(df.char, stringsAsFactors=F)
  
  df_out <- df.char %>% 
    mutate(
      Alley = ifelse(is.na(Alley), "None", Alley),
      MasVnrType = ifelse(is.na(MasVnrType), "None", MasVnrType),
      BsmtQual = ifelse(is.na(BsmtQual), "None", BsmtQual),
      BsmtCond = ifelse(is.na(BsmtCond), "None", BsmtCond),
      BsmtExposure = ifelse(is.na(BsmtExposure), "None", BsmtExposure),
      BsmtFinType1 = ifelse(is.na(BsmtFinType1), "None", BsmtFinType1),
      BsmtFinType2 = ifelse(is.na(BsmtFinType2), "None", BsmtFinType2),
      Electrical = ifelse(is.na(Electrical), "SBrkr", Electrical),
      FireplaceQu = ifelse(is.na(FireplaceQu), "None", FireplaceQu),
      GarageQual = ifelse(is.na(GarageQual), "None", GarageQual),
      GarageFinish = ifelse(is.na(GarageFinish), "None", GarageFinish),
      GarageType = ifelse(is.na(GarageType), "None", GarageType),
      GarageCond = ifelse(is.na(GarageCond), "None", GarageCond),
      PoolQC = ifelse(is.na(PoolQC), "None", PoolQC),
      Fence = ifelse(is.na(Fence), "None", Fence),
      MiscFeature = ifelse(is.na(MiscFeature), "None", MiscFeature),
      SaleType = ifelse(is.na(SaleType), "Oth", SaleType),
      Functional = ifelse(is.na(Functional), "Typ", Functional),
      KitchenQual = ifelse(is.na(KitchenQual), "TA", KitchenQual),
      Exterior1st = ifelse(is.na(Exterior1st), "VinylSd", Exterior1st),
      Exterior2nd = ifelse(is.na(Exterior2nd), "VinylSd", Exterior2nd),
      KitchenQual = ifelse(is.na(KitchenQual), "TA", KitchenQual),
      Utilities = ifelse(is.na(Utilities), "AllPub", Utilities),
      MSZoning = ifelse(is.na(MSZoning), "RL", MSZoning))
  
  df_return <- as.data.frame(lapply(df_out, as.factor))
  
  return(df_return)
}

factor_scale <- function(df_cat){
  
  df.numfac <- sapply(df_cat, as.numeric) # Converting factor to number
  df.numfacScale <- scale(df.numfac, scale = T, center = T) # Converting factor to number
  
  return(df.numfacScale)
}

dummify <- function(df_nodummy){
  
  dummies <- dummyVars("~.", data=df_nodummy)

  df.dmy <- data.frame(predict(dummies, newdata = df_nodummy))
  
  return(df.dmy)
}

kaggle_submit_format <- function(model, modnum) {
  filename <- paste0("output", modnum, ".csv")
  
  temp_df <- data.frame(test_ids, sapply(model, exp))
  colnames(temp_df) <- c("Id", "SalePrice")
  write.csv(temp_df, file = filename, row.names = F)
}

```


IMPORTING THE TRAINING AND TEST DATA
```{r}
housing <- read.csv("train.csv")
housing_test <- read.csv("test.csv")
```

```{r}
str(housing)
```

ANALYSIS OF MISSINGNESS
```{r}
VIM::aggr(housing)
```

EDA
```{r}
t <- theme_classic()

ggplot(housing, aes(x=OverallCond, y=SalePrice)) +
  geom_jitter(alpha=0.5) + t +
  geom_smooth(method="lm", se = F)

ggplot(housing, aes(x=OverallQual, y=SalePrice)) +
  geom_jitter(alpha=0.5) + t +
  geom_smooth(method="lm", se = F)

ggplot(housing, aes(x=GrLivArea, y=SalePrice)) +
  geom_jitter(alpha=0.5) + t + 
  geom_smooth(method="lm")

# When partitioning the data by neighborhood it looks more linear. 
ggplot(housing, aes(x=OverallQual, y=SalePrice)) +
  geom_jitter(alpha=0.5) + t +
  facet_wrap(~Neighborhood) #+ geom_smooth(method="lm", se = F)
```

```{r}
housing %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

REMOVAL OF EXTREME OUTLIERS
```{r}
housing <- housing %>% filter(GrLivArea < 4000)
```

STORING VECTORS TO BE USED LATER
```{r}
train_ids <- housing$Id
test_ids <- housing_test$Id
salesprice <- housing$SalePrice
sp <- sapply(salesprice, log)

housing <- housing[-81] # Removing SalePrice for model training
```

It looks like some of these features are nonparameterically distributed. Let's measure at the skewness of the distribution of the numerical data.
```{r}
numvars <- housing %>% keep(is.numeric)
# numvars <- numvars[,-c(1,3,9,25)]
```

```{r}
sk <- apply(numvars, 2, skewness)
sk
```

Some of these values are highly skewed

```{r}
# Takes in a dataframe of numerical values and applies Box-Cox Transformation to all columns 
bc_transform <- function(df){
  bcList <- apply(df, 2, BoxCoxTrans)
  transList <- purrr::map2(bcList, df, function(x,y){predict(x,y)})
  df_trans <- as.data.frame(do.call(cbind, transList))
  return(df_trans)
}

preprocess <- function(x, catvar_vec) {
  
  x <- x %>% column_to_rownames("Id")
  
  # Partitioning data for whether they are categorical or numerical for further preprocessing
  x_cat <- x %>% keep(is.factor) 
  x_int <- x %>%  keep(is.numeric) 
  
  x_int.imp <- kNN(x_int, k = 5) # KNN imputation of numerical data
  x_int.imp <- x_int.imp[, 1:37]
  
  x_cat <- x_cat %>% mutate(HasBsmt = ifelse(is.na(BsmtQual), 0, 1), # New feature "Is there a basement or not?"
                            HasFireplace = ifelse(is.na(FireplaceQu), 0, 1), # New features: "Is there a fireplace or not?"
                            HasAlley = ifelse(is.na(Alley), 0, 1), # New features: "Is there an alleyway or not?"
                            HasFence = ifelse(is.na(Fence), 0, 1),  # New features: "Does the house have a fence?"
                            HasGarage = ifelse(is.na(GarageQual), 0, 1), # New feature: "Does the house have a garage?"
                            HasPool = ifelse(is.na(PoolQC), 0, 1)) # New feature: "Does the house have a pool?"
  
  x_cat.imp <- cat_imputation(x_cat) # Imputing the categorical data in accordance with dataset documentation.
  
  x.ordinal <- x_cat.imp[catvar_vec] # Remove the categorical variables from the numerical variables, leaving the 1-hot encoded new features
  
  # Imputes the categorical variables according to the dataset documentation.
  x.ordinal <- sapply(x.ordinal, function(x) case_when(
    x == "Ex" ~ 5,
    x == "Gd" ~ 4,
    x == "TA" ~ 3,
    x == "Fa" ~ 2,
    x == "Po" ~ 1,
    x == "None" ~ 0
  ))
  
  
  #Selecting all categoricals not in the ordinal list
  x.truecat <- x_cat.imp[!(names(x_cat.imp) %in% catvar_vec)]
  
  x_numerical <- cbind(x_int.imp[,-37], x.ordinal)
  x_numerical <- x_numerical %>% 
                      mutate(Has2ndFl = ifelse(X2ndFlrSF==0, 0, 1), # New Feature: "Is this a 2-story house"?
                             TotBaths = FullBath + HalfBath, # New feature: "How many baths does this house have"?
                             TotSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF + GarageArea) # Total SF of the house and garage
  x.truecat$Has2ndFl <- sapply(x_numerical$Has2ndFl, as.factor) # Converting from number to factor
  x_numerical$Has2ndFl <- NULL
  
  #Apply Box-Cox transformation to all columns of the numerical dataframe
  x_numerical_bc <- bc_transform(x_numerical)
  
  x_numerical_bcScaled <- as.data.frame(scale(x_numerical_bc)) #Renorming and scaling the numerical variables.
  
  x_fullData <- cbind(x_numerical_bcScaled, x.truecat) # Recombining numerical and categorical data.
  
  return(x_fullData)
}

```


```{r}
# preprocess_cat <- function(x) {
#   
#   # Contains many of the same features as preprocess
#   
#   x_cat <- x %>% keep(is.factor) 
#   
#   x_cat <- x_cat %>% mutate(HasBsmt = ifelse(is.na(BsmtQual), 0, 1),
#                             HasFireplace = ifelse(is.na(FireplaceQu), 0, 1), 
#                             HasAlley = ifelse(is.na(Alley), 0, 1), 
#                             HasFence = ifelse(is.na(Fence), 0, 1), 
#                             HasGarage = ifelse(is.na(GarageQual), 0, 1), 
#                             HasPool = ifelse(is.na(PoolQC), 0, 1))
#   
#   x_cat.imp <- cat_imputation(x_cat)
#   
#   x.ordinal <- x_cat.imp[catvar_vec]
#   
#   x.ordinal <- sapply(x.ordinal, function(x) case_when(
#     x == "Ex" ~ 5,
#     x == "Gd" ~ 4,
#     x == "TA" ~ 3,
#     x == "Fa" ~ 2,
#     x == "Po" ~ 1,
#     x == "None" ~ 0
#   ))
#   
#   #Selecting all categoricals not in the ordinal list
#   x.truecat <- x_cat.imp[!(names(x_cat.imp) %in% catvar_vec)]
#   
#   x_numerical <- cbind(x_int.imp[,-37], x.ordinal)
#   x_numerical <- x_numerical %>% 
#                       mutate(Has2ndFl = ifelse(X2ndFlrSF==0, 0, 1), 
#                              TotBaths = FullBath + HalfBath, 
#                              TotSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF + GarageArea) # Total SF of the house and garage
#   x.truecat$Has2ndFl <- sapply(x_numerical$Has2ndFl, as.factor)
#   x_numerical$Has2ndFl <- NULL
#   
#   x_numerical <- as.data.frame(scale(x_numerical))
#   
#   x_fullData <- cbind(x_numerical, x.truecat)
#   
#   return(x_fullData)
# }
```


IMPUTATION OF CATEGORICAL VARIABLES (TRAINING)
```{r}
catvar_vec <- c('ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC')

housing.processed <- preprocess(housing, catvar_vec)

housing.processed <- cbind(housing.processed, sp)
```

IMPUTATION OF CATEGORICAL VARIABLES (TESTING)
```{r}
housing_test.processed <- preprocess(housing_test, catvar_vec)
```

LASSO (for feature selection)
```{r}
housing.num <- housing.processed %>% keep(is.numeric)

x = model.matrix(sp ~ ., housing.num)[, -1] #Dropping the intercept column.
y = housing.num$sp

grid = 10^seq(3, -6, length = 100)
```

```{r}
lasso.models.housingint = glmnet(x, y, alpha = 1, lambda = grid)
```

```{r}
coef(lasso.models.housingint)
```

```{r}
plot(lasso.models.housingint, xvar = "lambda", label = TRUE, main = "Lasso Regression")
```

Exporting LASSO features from analysis output
```{r}
features <- coef(lasso.models.housingint)[-1, 90]
features <- sort(abs(features), decreasing = T)
features <- features[features>0]

feature_names <- names(features)
feature_names
```

#### Feature Selection of Categorical Variables by Random Forest ####
```{r}
housing.factors <- housing.processed %>% keep(is.factor)

housing_train_proc6 <- cbind(housing.factors, sp)

# droplist <- c("Utilities", "HouseStyle", "Condition2", "RoofMatl", "Exterior1st","Exterior2nd", "Heating", "Electrical", "MiscFeature")

# housing_train_proc6 <- housing_train_proc6[!(colnames(housing_train_proc6) %in% droplist)]
```

```{r}
set.seed(314)
rf.housing <- randomForest(sp ~., 
                           data=housing_train_proc6,
                           importance = T)
rf.housing
plot(rf.housing)
```

```{r}
set.seed(0)
oob.err = numeric(25)
for (mtry in 1:25) {
  fit = randomForest(sp ~ ., data = housing_train_proc6, mtry = mtry)
  oob.err[mtry] = fit$mse[500]
  cat("Running iteration ", mtry, "\n")
}
```

```{r}
plot(1:25, oob.err, pch = 16, type = "b",
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")
```

```{r}
importance(rf.housing)
varImpPlot(rf.housing)
```

```{r}
rf.sigvars <- names(sort(importance(rf.housing)[,1], decreasing = T))
```

```{r}
rf.rankedvars <- sort(importance(rf.housing)[,1], decreasing = T) 
rf.sigvars <- names(rf.rankedvars[rf.rankedvars >15]) # Names of vars with %IncMSE
```

ONE-HOT ENCODING OF FACTOR VARIABLES FOR LIMITED DATASETS

Train
```{r}
droplist <- c("Utilities", "HouseStyle", "Condition2", "RoofMatl", "Exterior1st","Exterior2nd", "Heating", "Electrical", "MiscFeature")

# Note that dplyr select function is being masked by another package and is explicitly stated here.
housing_catFeatures <- housing.processed %>% keep(is.factor) %>% dplyr::select(rf.sigvars) %>% dplyr::select(-one_of(droplist))
housing_cat.dmy6 <- dummify(housing_catFeatures)

housing_int.features <- housing.processed %>% keep(is.numeric) %>% dplyr::select(feature_names)

housing_train_proc <- cbind(housing_int.features, housing_cat.dmy6)
```
Test
```{r}
housing_test_catFeatures <- housing_test.processed %>% keep(is.factor) %>% dplyr::select(rf.sigvars) %>% dplyr::select(-one_of(droplist))
housing_test_cat.dmy6 <- dummify(housing_test_catFeatures)

housing_test_int.features <- housing_test.processed %>% keep(is.numeric) %>% dplyr::select(feature_names)

housing_test_proc <- cbind(housing_test_int.features, housing_test_cat.dmy6)
```


ONE-HOT ENCODING OF FACTOR VARIABLES FOR FULL DATASETS

Train
```{r}
# droplist <- c("Utilities", "HouseStyle", "Condition2", "RoofMatl", "Exterior1st","Exterior2nd", "Heating", "Electrical", "MiscFeature")

housing_catsub <- housing.processed %>% keep(is.factor) %>% dplyr::select(rf.sigvars) %>% dplyr::select(-one_of(droplist))
housing_cat.dmyFull <- dummify(housing_catsub)

housing_intFull <- housing.processed %>% keep(is.numeric) 

housing_train_Full <- cbind(housing_intFull, housing_cat.dmyFull)
```

Test
```{r}
housing_test_catsub <- housing_test.processed %>% keep(is.factor) %>% dplyr::select(rf.sigvars) %>% dplyr::select(-one_of(droplist))
housing_test_cat.dmyFull <- dummify(housing_test_catsub)

housing_test_intFull <- housing_test.processed %>% keep(is.numeric) 

housing_test_Full <- cbind(housing_test_intFull, housing_test_cat.dmyFull)
```

#### KNN Model ####

```{r}
housing_train_proc6 <- cbind(housing_train_proc, sp)
housing_test_proc6 <- cbind(housing_test_proc)
```

```{r}
trControl <- trainControl(method = "repeatedcv", 
                          number=10, 
                          repeats=3)
```

```{r}
set.seed(314)
knn_mod6 <- train(sp ~ .,
                  data = housing_train_pca, 
                  tuneGrid=expand.grid(k=1:30),
                  method = "knn", 
                  tuneLength = 20, 
                  trControl = trControl)
```

```{r}
knn_mod6
```

```{r}
plot(knn_mod6)
```

```{r}
varImp(knn_mod6)
```


```{r}
pred_knn6 <- predict(knn_mod6, newdata=housing_test_pca)  
```

```{r}
kaggle_submit_format(pred_knn6, 6)
```


```{r}
output6 <- data.frame(train_ids, sapply(pred_knn6, exp))
colnames(output6) <- c("Id", "SalePrice")
```

```{r}
write.csv(output6, file="output6.csv", row.names = F)
```

```{r}
saveRDS(knn_mod6, "knn_housing.rds")
```

#### PCA Dimension Reduction and KNN Model ####

```{r, warning=F, echo=F}
library(psych)
```

```{r}
trainKNN_cov <- cov(housing_train_proc)
```


```{r}

fa.parallel(trainKNN_cov, 
            n.obs = dim(housing_train_proc)[1], 
            fa="pc", 
            n.iter = 100)
abline(h=1)
```
```{r}
pc_KNN <- principal(trainKNN_cov, 
                         nfactors = 8, 
                         rotate = 'none')
pc_KNN
```
```{r}
# factor.plot(pc_KNN,
#             labels = colnames(trainKNN_cov))
```

Applying the PCA model to reduce the dimensionality of the train and test datasets
```{r}
train_pca <- predict(pc_trainKNN, housing_train_proc, housing_train_proc)
test_pca <- predict(pc_trainKNN, housing_test_proc, housing_train_proc)
```

```{r}
housing_train_pca <- cbind(train_pca, sp)
housing_test_pca <- cbind(test_pca)
```

==================================
```{r}
trControl <- trainControl(method = "repeatedcv", 
                          number=10, 
                          repeats=3)
```

```{r}
set.seed(314)
knnPCA_mod <- train(sp ~ .,
                  data = housing_train_pca, 
                  tuneGrid=expand.grid(k=1:30),
                  method = "knn", 
                  tuneLength = 20, 
                  trControl = trControl)
```

```{r}
knnPCA_mod
```

```{r}
plot(knnPCA_mod)
```

```{r}
varImp(knnPCA_mod)
```


```{r}
pred_knnPCA <- predict(knnPCA_mod, newdata=housing_test_pca)  
```

```{r}
kaggle_submit_format(pred_knnPCA, 6)
```


```{r}
output6 <- data.frame(train_ids, sapply(pred_knnPCA, exp))
colnames(output6) <- c("Id", "SalePrice")
```

```{r}
write.csv(output6, file="output6.csv", row.names = F)
```

```{r}
saveRDS(pred_knnPCA, "knnPCA_housing.rds")
```


#### RANDOM FOREST ####

```{r}
library(randomForest)
```

```{r}
housing_train_rf <- housing.processed %>% dplyr::select(-one_of(droplist))
housing_test_rf <- housing_test.processed %>% dplyr::select(-one_of(droplist))
```


```{r}
set.seed(314)
rf.housing2 <- randomForest(sp ~., 
                           data=housing_train_rf,
                           importance = T,
                           mtry=15, 
                           ntree=1000)
rf.housing2
```

```{r}
plot(rf.housing2)
```


```{r}
set.seed(0)
oob.err = numeric(50)
for (mtry in 1:45) {
  fit = randomForest(sp ~ ., data = housing_train_rf, mtry = mtry)
  oob.err[mtry] = fit$mse[500]
  cat("Running iteration ", mtry, "\n")
}
```

```{r}
plot(1:50, oob.err, pch = 16, type = "b",
     xlab = "Variables Considered at Each Split",
     ylab = "OOB Mean Squared Error",
     main = "Random Forest OOB Error Rates\nby # of Variables")
```

```{r}
importance(rf.housing2)
varImpPlot(rf.housing2)
```

```{r}
pred_rf2 <- predict(rf.housing2, newdata = housing_test_rf, predict.all=T)
```

```{r}
kaggle_submit_format(pred_rf2$aggregate, 7)
```

```{r}
output7 <- data.frame(train_ids, sapply(pred_rf2$aggregate, exp))
colnames(output7) <- c("Id", "SalePrice")
```

```{r}
write.csv(output7, file="output7.csv", row.names = F)
```

```{r}
saveRDS(rf.housing2, "RF_housing.rds")
```


#### Ridge Regression #### (MODEL 9)

```{r}
library(glmnet)
set.seed(0)
grid <- 10^seq(5, -2, length = 100)

x <-  model.matrix(sp ~ ., housing_train_Full)[, -1] #Dropping the intercept column.
y <-  housing_train_Full$sp

ridge.housing = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.housing))
coef(ridge.housing) 
```



#Visualizing the ridge regression shrinkage.
```{r}
plot(ridge.housing, xvar = "lambda", label = TRUE, main = "Ridge Regression")
```


Running 10-fold cross validation.
```{r}
set.seed(0)
cv.ridge.out = cv.glmnet(x, y,
                         lambda = grid, alpha = 0, nfolds = 10)
plot(cv.ridge.out, main = "Ridge Regression\n")
bestlambda.ridge = cv.ridge.out$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
```




#Here the MSE is lower at approximately 113,173; a further improvement
#on that which we have seen above. With "cv.ridge.out", we can actually access
#the best model from the cross validation without calling "ridge.models.train"
#or "bestlambda.ridge":
```{r}
ridge.bestlambdatrain <-  predict.cv.glmnet(cv.ridge.out, s ="lambda.min", newx = as.matrix(housing_test_Full))
```

```{r}
kaggle_submit_format(ridge.bestlambdatrain, 9)
```

```{r}
output9 <- data.frame(train_ids, sapply(ridge.bestlambdatrain, exp))
colnames(output9) <- c("Id", "SalePrice")
```

```{r}
write.csv(output9, file="output9.csv", row.names = F)
```

```{r}
saveRDS(cv.ridge.out, "ridge_optimized.rds")
```


#### SVR #### (Model 10)

```{r}
svr.housing <- svm(sp ~ ., data=housing_train_Full)
```

```{r}
tuneResult <- tune(svm, sp ~ .,  data = housing_train_Full,
              ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9))
)
print(tuneResult)

# Draw the tuning graph
plot(tuneResult)
```

```{r}
svr.housing.tuned <- tuneResult$best.model
```

```{r}
pred_svr <- predict(svr.housing.tuned, housing_test_Full)
```

```{r}
kaggle_submit_format(pred_svr, 10)
```

```{r}
output10 <- data.frame(train_ids, sapply(pred_svr, exp))
colnames(output10) <- c("Id", "SalePrice")
```

```{r}
write.csv(output10, file="output10.csv", row.names = F)
```

```{r}
saveRDS(svr.housing.tuned, "svr_tuned.rds")
```


#### Neural Network #### 

```{r}
nn_features <- names(housing_train_Full[, -49])

f <- paste(nn_features,collapse=' + ')
f <- paste('sp ~',f)
f
```

```{r}
nn <-
  neuralnet(
    f,
    housing_train_Full,
    hidden = c(30, 10),
    linear.output = T,
    stepmax = 1e6,
    threshold = 0.1, 
    algorithm="backprop", 
    learningrate=0.0001,
    err.fct = "sse",
    rep=3
  )
```

```{r}
plot(nn)

dev.print(pdf, 'nn.pdf', width=8, height=20) 
```

```{r}
# Compute Predictions off Test Set
predicted.nn.values <- compute(nn, housing_train_Full[-49]) 

pr.nn_ <- predicted.nn.values$net.result*(max(housing_train_Full$sp)-min(housing_train_Full$sp))+min(housing_train_Full$sp)

# Check out net.result
print(head(predicted.nn.values$net.result))
```

```{r}
kaggle_submit_format(predicted.nn.values$net.result, 12)
```

```{r}
output <- data.frame(train_ids, sapply(predicted.nn.values$net.result, exp))
colnames(output) <- c("Id", "SalePrice")
```

```{r}
write.csv(output, file="output12t.csv", row.names = F)
```

```{r}
saveRDS(nn, "nn.rds")
```


#### Visualization of Training data ####

```{r}
pred_train <- read.csv("predictions_training.csv")
pred_train <- cbind(pred_train, salesprice)

pred_test <- read.csv("predictions_test.csv")
```


#### ENSEMBLE MODEL 1 #### (Simple Average)
```{r}
pred_test3 <- read.csv("predictions_test.csv")
ensemble_id_test <- pred_test3$Id
pred_test3 <- pred_test3[,-1]
```

```{r}
pred_train3 <- read.csv("predictions_training.csv")
pred_train <- pred_train3[,-1]
```

```{r}
pred_means <- apply(pred_test3, 1, mean)
```

```{r}
test_df <- data.frame(pred_means, salesprice)
```


```{r}
output <- data.frame(ensemble_id_test, pred_means)
colnames(output) <- c("Id", "SalePrice")
```

```{r}
write.csv(output, file="ensemble_means.csv", row.names = F)
```


#### ENSEMBLE MODEL 2 ####

```{r}
metric <- "RMSE"
trainControl_e <- trainControl(method="cv", number=10)

set.seed(99)
gbm.caret <- train(salesprice ~ .,
                   data=pred_train,
                   distribution="gaussian",
                   method="gbm",
                   trControl=trainControl_e,
                   verbose=FALSE,
                   # tuneGrid=caretGrid,
                   metric=metric,
                   bag.fraction=0.75,
                   )                  


emsemble.GBMpredict <- predict(gbm.caret, newdata=pred_test, type="raw")

```

```{r}
print(gbm.caret)
```


```{r}
output <- data.frame(ensemble_id_test, emsemble.GBMpredict)
colnames(output) <- c("Id", "SalePrice")
```

```{r}
write.csv(output, file="ensemble_GBM.csv", row.names = F)
```